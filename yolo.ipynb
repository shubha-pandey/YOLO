{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YOLO\n",
    "\n",
    "YOLO (You Only Look Once) is a real-time object detection algorithm developed by Joseph Redmon and Ali Farhadi in 2015. It is a single-stage object detector that uses a convolutional neural network (CNN) to predict the bounding boxes and class probabilities of objects in input images. YOLO was first implemented using the Darkent framework.\n",
    "\n",
    "The YOLO algorithm divides the input image into a grid of cells, and for each cell, it predicts the probability of the presence of an object and the bounding box coordinates of the object. It also predicts the class of the object. Unlike two-stage object detectors such as R-CNN and its variants, YOLO processes the entire image in one pass, making it faster and more efficient.\n",
    "\n",
    "\n",
    "The basic idea behind YOLO is to divide the input image into a grid of cells and, for each cell, predict the probability of the presence of an object and the bounding box coordinates of the object. The process of YOLO can be broken down into several steps:\n",
    "\n",
    "1. Input image is passed through a CNN to extract features from the image.\n",
    "\n",
    "2. The features are then passed through a series of fully connected layers, which predict ‌class probabilities and bounding box coordinates.\n",
    "\n",
    "3. The image is divided into a grid of cells, and each cell is responsible for predicting a set of bounding boxes and class probabilities.\n",
    "\n",
    "4. The output of the network is a set of bounding boxes and class probabilities for each cell.\n",
    "\n",
    "5. The bounding boxes are then filtered using a post-processing algorithm called non-max suppression to remove overlapping boxes and choose the box with the highest probability.\n",
    "\n",
    "6. The final output is a set of predicted bounding boxes and class labels for each object in the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "\n",
    "\n",
    "img = cv2.imread('OD_Assets/bus_students.jpg')\n",
    "img = cv2.resize(img, (600,400))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOLOv8\n",
    "\n",
    "YOLOv8 is the newest model in the YOLO algorithm series – the most well-known family of object detection and classification models in the Computer Vision (CV) field. With the latest version, the YOLO legacy lives on by providing state-of-the-art results for image or video analytics, with an easy-to-implement framework.\n",
    "\n",
    "YOLOv8 is a state-of-the-art deep learning model designed for real-time object detection in computer vision applications. With its advanced architecture and cutting-edge algorithms, YOLOv8 has revolutionized the field of object detection, enabling accurate and efficient detection of objects in real-time scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classNames = [\n",
    "    \"person\", \"bicycle\", \"car\", \"motorcycle\", \"airplane\", \"bus\", \"train\", \"truck\", \n",
    "    \"boat\", \"traffic light\", \"fire hydrant\", \"stop sign\", \"parking meter\", \"bench\", \n",
    "    \"bird\", \"cat\", \"dog\", \"horse\", \"sheep\", \"cow\", \"elephant\", \"bear\", \"zebra\", \"giraffe\", \"backpack\", \"umbrella\", \"handbag\", \"tie\", \"suitcase\", \"frisbee\",\n",
    "    \"skis\", \"snowboard\", \"sports ball\", \"kite\", \"baseball bat\", \"baseball glove\",\n",
    "    \"skateboard\", \"surfboard\", \"tennis racket\", \"bottle\", \"wine glass\", \"cup\",\n",
    "    \"fork\", \"knife\", \"spoon\", \"bowl\", \"banana\", \"apple\", \"sandwich\", \"orange\",\n",
    "    \"broccoli\", \"carrot\", \"hot dog\", \"pizza\", \"donut\", \"cake\", \"chair\", \"couch\",\n",
    "    \"potted plant\", \"bed\", \"dining table\", \"toilet\", \"TV\", \"laptop\", \"mouse\",\n",
    "    \"remote\", \"keyboard\", \"cell phone\", \"microwave\", \"oven\", \"toaster\", \"sink\",\n",
    "    \"refrigerator\", \"book\", \"clock\", \"vase\", \"scissors\", \"teddy bear\", \"hair drier\",\n",
    "    \"toothbrush\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Creating the model\n",
    "\n",
    "[Using default version]\n",
    "\n",
    "There are different types of weights, the nano, the medium, the large.\n",
    "Nano will be faster, Medium will be a bit slower and Large will be slower.\n",
    "It's based upon us to decide which to use. \n",
    "\n",
    "\n",
    "_Speed:_  Nano  >  Medium  >  Large\n",
    "\n",
    "_Accuracy:_  Nano  <  Medium  <  Large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NANO\n",
    "\n",
    "model = YOLO('YOLO_Weights/yolov8n.pt')           # state the weights needed and it will download              yolo version 8 n for nano\n",
    "\n",
    "# 448x640 5 persons, 1 bus, 1 backpack, 1 handbag, 121.5ms Speed: 8.7ms preprocess, 121.5ms inference, 0.0ms postprocess per image at shape (1, 3, 448, 640)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MEDIUM\n",
    "\n",
    "#model = YOLO('YOLO_Weights/yolov8m.pt')           # 448x640 5 persons, 1 bus, 5 backpacks, 1 handbag, 1 book, 465.9ms Speed: 3.6ms preprocess, 465.9ms inference, 4.3ms postprocess per image at shape (1, 3, 448, 640)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LARGE\n",
    "\n",
    "#model = YOLO('YOLO_Weights/yolov8l.pt')            # 448x640 5 persons, 1 bus, 5 backpacks, 1 handbag, 2 books, 910.4ms Speed: 8.0ms preprocess, 910.4ms inference, 0.0ms postprocess per image at shape (1, 3, 448, 640)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model(img, show=True)\n",
    "\n",
    "results = model('OD_Assets/cars.jpg', show=True)    #  448x640 20 cars, 121.8ms Speed: 7.0ms preprocess, 121.8ms inference, 0.0ms postprocess per image at shape (1, 3, 448, 640)\n",
    "\n",
    "\n",
    "# The closer is the object to the camera the better it is detected.\n",
    "# Basically the idea is how clear is the site of viewing.\n",
    "\n",
    "\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  YOLO WITH WEBCAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cvzone                                       # used to display all the dtections in a little bit easier manner\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up webcam \n",
    "\n",
    "cap = cv2.VideoCapture(0)                           # for webcam\n",
    "cap.set(3, 640)\n",
    "cap.set(4, 480)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for video files\n",
    "\n",
    "#cap = cv2.VideoCapture('OD_Assets/people.mp4')                          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True :\n",
    "    success, frame = cap.read()\n",
    "\n",
    "    #frame = cv2.resize(frame, (640, 480))           # resizing the video file \n",
    "\n",
    "    results = model(frame, stream=True)             # stream=True use generators which is more efficient \n",
    "    \n",
    "    for result in results :\n",
    "        boxes = result.boxes\n",
    "        for box in boxes :\n",
    "            # OPENCV\n",
    "            \n",
    "            x1, y1, x2, y2 = box.xyxy[0]\n",
    "            #print(x1, y1, x2, y2)\n",
    "\n",
    "            # converting x1, y1, x2, y2 values into intergesrs in order to read them\n",
    "            x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2)\n",
    "    \n",
    "            #print(x1, y1, x2, y2)\n",
    "\n",
    "            cv2.rectangle(frame, (x1, y1), (x2, y2), (255, 0, 255), 3)\n",
    "           \n",
    "            # CVZONE\n",
    "\n",
    "            #w, h = x2-x1, y2-y1\n",
    "            #cvzone.cornerRect(frame, (x1, y1, w, h))\n",
    "            \n",
    "            # finding confidence\n",
    "            conf = math.ceil((box.conf[0]*100))/100\n",
    "            #print(conf)\n",
    "\n",
    "            # displaying confidence and class name            \n",
    "            cls = int(box.cls[0])\n",
    "            cvzone.putTextRect(frame, f'{classNames[cls]} {conf}', (max(0, x1), max(35, y1)), scale=1, thickness=2)\n",
    "    \n",
    "    cv2.imshow(\"Image\", frame)\n",
    "            \n",
    "    if cv2.waitKey(1) & 0xFF == ord('x') :\n",
    "        break\n",
    "\n",
    "\n",
    "\n",
    "# This is how webcam is used with the YOLOv8 to detect different objects.\n",
    "# Using it with CPU, (that's why the large version is very slow). Quite inefficient!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  --- YOLO WITH GPU  ---\n",
    "\n",
    "# Install CUDA Toolkit and CUDNN Toolkit  --> copy CUDNN files into CUDA  --->  ensure the path is correct in environment variables  --> Restart\n",
    "# CUDA 11.8 and CUDNN 8.6.0 for CUDA 11.x\n",
    "# Install compatible torch and torchvision     pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "\n",
    "\n",
    "# TODO: maybe try installing cuda 11.2 and cudnn 8.1 , tensorflow 2.10"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
